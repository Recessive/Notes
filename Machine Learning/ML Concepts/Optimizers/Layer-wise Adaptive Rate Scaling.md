---
aliases: [LARS]
---
# Layer-wise Adaptive Rate Scaling Optimizer

[Paper](https://arxiv.org/abs/1708.03888v3)
[Github](https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11)

Uses a sperate learning rate for each *layer* instead of each *[[ML Glossary#Parameter|parameter]]* like [[Adaptive Moment Estimation|Adam]] does.

Lots of mathss, don't have time

#machine-learning
#optimizer
#incomplete