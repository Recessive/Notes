# Contrastive Learning
A method of learning that attempts to train a network to clump similar classes and separate dissimilar classes. Typically uses [[Triplet Loss]] to train the network, which essentially means 3 samples (an **anchor, positive** and **negative**) are passed through the network, and the loss calculated based on that triplet.

As an interesting aside, this method is inspired by human learning patterns. Humans have a much easier time learning if they are able to compare a sample to others.

(Note: pairs can be used instead of triplets with [[Contrastive Loss]], however for succinctness, I will just assume [[Triplet Loss]] is being used)

## Supervised
Labels are available in training, so generating triplets is easy. However, generating *all* possible triplets is much harder. There's a ridiculous amount for a large dataset. On top of that, looking at [[Triplet Loss]], there are an ever increasing number of triplets that already satisfy the distance threshold ($m$) and therefor output 0 loss.

Therefor, we have to generate **hard triplets**, which means their loss value is high. Obviously the only way to test this is to actually run it through the network, so we need a way of finding **hard triplets** *without running them through the network*. This process is called **Triplet Mining** (or some close spin on that term).

For example in an NLP task a hard triplet can be generated by negating the **anchor** sample (ie, adding a "not" to the sentence).

## Unsupervised
More commonly referred to as **Self-Supervised** in the context of **Contrastive Learning**

