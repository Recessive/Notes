# Contrastive Learning
A method of learning that attempts to train a network to clump similar classes and separate dissimilar classes. Typically uses [[Triplet Loss]] to train the network in the [[Contrastive Learning#Supervised|supervised case]], and [[Contrastive Loss]] in the [[Contrastive Learning#Unsupervised|unsupervised case]]

As an interesting aside, this method is inspired by human learning patterns. Humans have a much easier time learning if they are able to compare a sample to others.



## Supervised
(Note: pairs can be used instead of triplets with [[Contrastive Loss]], however for succinctness, I will just assume [[Triplet Loss]] is being used)

Labels are available in training, so generating triplets is easy. However, generating *all* possible triplets is much harder. There's a ridiculous amount for a large dataset. On top of that, looking at [[Triplet Loss]], there are an ever increasing number of triplets that already satisfy the distance threshold ($m$) and therefor output 0 loss.

Therefor, we have to generate **hard triplets**, which means their loss value is high. Obviously the only way to test this is to actually run it through the network, so we need a way of finding **hard triplets** *without running them through the network*. This process is called **Triplet Mining** (or some close spin on that term).

For example in an NLP task a hard triplet can be generated by negating the **anchor** sample (ie, adding a "not" to the sentence).

## Unsupervised
More commonly referred to as **Self-Supervised** in the context of **Contrastive Learning**

Labels are not available, and so they must be synthesised. The most common way of doing this is to have positive labels be some augmentations of a sample in the training data, and negative labels be all other samples.

**A bit simpler put:**
Generally, one training sample will be trained on a bunch of augmentations of itself as positive, and all other samples as negative

A good example of this is the [[SimCLR]] network, that uses [[Normalized Temperature-scaled Cross Entropy Loss]] (basically log [[Softmax|softmax]] loss)
