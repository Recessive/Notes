# Self attention
Self attention is a subset of [[Attention]]. It essentially boils down to the **key** and **query** in the [[Attention]] mechanism deriving from the current layer. To be more clear, below is the diagram included in [[Attention]], but modified to show how Self attention works:

![[Self attention drawing 1|900]]

#machine-learning
#concept
#attention